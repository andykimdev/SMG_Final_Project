{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Transformer Cancer Classification Demo\n",
        "\n",
        "This notebook demonstrates the Graph Transformer model for cancer type classification:\n",
        "- Model architecture and training\n",
        "- Model evaluation\n",
        "- Full interpretability analysis (SHAP, attention, PCA baseline)\n",
        "- Comparative analysis and result generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/leophelan/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code\n",
            "PyTorch version: 2.8.0\n",
            "Working directory: /Users/leophelan/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Determine project root (notebook is in notebooks/ subdirectory)\n",
        "# In Jupyter, __file__ doesn't exist, so use getcwd()\n",
        "cwd = Path(os.getcwd())\n",
        "if (cwd / \"src\").exists() and (cwd / \"interpretability\").exists():\n",
        "    PROJECT_ROOT = cwd\n",
        "elif (cwd.parent / \"src\").exists() and (cwd.parent / \"interpretability\").exists():\n",
        "    PROJECT_ROOT = cwd.parent\n",
        "else:\n",
        "    # Fallback: assume we're in notebooks/ directory\n",
        "    PROJECT_ROOT = Path().resolve().parent\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"interpretability\"))\n",
        "\n",
        "import config\n",
        "from model import GraphTransformerClassifier\n",
        "from graph_prior import load_graph_prior, get_graph_features_as_tensors\n",
        "from dataset import load_and_preprocess_data, create_dataloaders\n",
        "from utils import load_trained_model, load_data, get_output_dirs, DEFAULT_PATHS\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Working directory: {cwd}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Availability Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data files found - model can be trained\n",
            "  CSV: /Users/leophelan/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/data/processed_datasets/tcga_pancan_rppa_compiled.csv\n",
            "  Prior: /Users/leophelan/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/data/priors/tcga_string_prior.npz\n"
          ]
        }
      ],
      "source": [
        "def check_data_availability():\n",
        "    \"\"\"Check if required data files are available.\"\"\"\n",
        "    data_dir = PROJECT_ROOT / \"data\"\n",
        "    csv_path = data_dir / \"processed_datasets\" / \"tcga_pancan_rppa_compiled.csv\"\n",
        "    prior_path = data_dir / \"priors\" / \"tcga_string_prior.npz\"\n",
        "    \n",
        "    # Also check alternative locations\n",
        "    if not csv_path.exists():\n",
        "        csv_path = data_dir / \"tcga_pancan_rppa_compiled.csv\"\n",
        "    if not prior_path.exists():\n",
        "        prior_path = data_dir / \"tcga_string_prior.npz\"\n",
        "    \n",
        "    csv_available = csv_path.exists()\n",
        "    prior_available = prior_path.exists()\n",
        "    \n",
        "    return csv_available and prior_available, csv_path if csv_available else None, prior_path if prior_available else None\n",
        "\n",
        "DATA_AVAILABLE, CSV_PATH, PRIOR_PATH = check_data_availability()\n",
        "\n",
        "if DATA_AVAILABLE:\n",
        "    print(\"✓ Data files found - model can be trained\")\n",
        "    print(f\"  CSV: {CSV_PATH}\")\n",
        "    print(f\"  Prior: {PRIOR_PATH}\")\n",
        "else:\n",
        "    print(\"⚠ Data files not found - will display model architecture only\")\n",
        "    print(\"  Place data files in data/processed_datasets/ and data/priors/\")\n",
        "    print(\"  See data/README.md for instructions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Architecture Display / Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model from scratch...\n",
            "Loaded prior: 198 proteins, 1184 edges\n",
            "Loading data from /Users/leophelan/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/data/processed_datasets/tcga_pancan_rppa_compiled.csv...\n",
            "Loaded 7523 samples\n",
            "Found 198 protein columns in CSV\n",
            "Filtering samples: 7523/7523 have ≤50.0% missing values\n",
            "After filtering: 7523 samples across 32 cancer types\n",
            "Cancer type distribution:\n",
            "CANCER_TYPE_ACRONYM\n",
            "ACC      45\n",
            "BLCA    343\n",
            "BRCA    876\n",
            "CESC    166\n",
            "CHOL     30\n",
            "COAD    346\n",
            "DLBC     33\n",
            "ESCA    125\n",
            "GBM     231\n",
            "HNSC    211\n",
            "KICH     62\n",
            "KIRC    455\n",
            "KIRP    209\n",
            "LGG     428\n",
            "LIHC    179\n",
            "LUAD    360\n",
            "LUSC    317\n",
            "MESO     63\n",
            "OV      414\n",
            "PAAD    122\n",
            "PCPG     79\n",
            "PRAD    350\n",
            "READ    118\n",
            "SARC    218\n",
            "SKCM    329\n",
            "STAD    354\n",
            "TGCT    118\n",
            "THCA    369\n",
            "THYM     90\n",
            "UCEC    423\n",
            "UCS      48\n",
            "UVM      12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data splits:\n",
            "  Train: 6394 samples (85.0%)\n",
            "  Val:   751 samples (10.0%)\n",
            "  Test:  378 samples (5.0%)\n",
            "\n",
            "Handling missing values...\n",
            "  train: 38652 missing values, imputing with train means\n",
            "  val: 4510 missing values, imputing with train means\n",
            "  test: 2291 missing values, imputing with train means\n",
            "\n",
            "Standardizing protein features...\n",
            "  Mean: [-0.72214299  0.3119937   0.21089168  0.78799248  1.543347  ] ... (first 5)\n",
            "  Std:  [0.27372869 0.55241333 0.35098655 0.761766   0.66660206] ... (first 5)\n",
            "Model parameters: 831,592\n",
            "\n",
            "Training (limited epochs for demo)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/100 [00:00<?, ?it/s]/Users/leophelan/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
            "/Users/leophelan/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss=3.2258, Val Acc=0.1651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/100 [00:00<?, ?it/s]/Users/leophelan/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Training:  72%|███████▏  | 72/100 [00:53<00:20,  1.35it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m model.train()\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[32m5\u001b[39m, config.TRAINING[\u001b[33m'\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m'\u001b[39m])):\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     train_loss, train_acc = \u001b[43mtrain_epoch_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     val_metrics = evaluate_simple(model, val_loader, criterion, \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_epoch_simple\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     41\u001b[39m x, y = x.to(device), y.to(device)\n\u001b[32m     42\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m loss = criterion(logits, y)\n\u001b[32m     45\u001b[39m loss.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/src/model.py:111\u001b[39m, in \u001b[36mGraphTransformerClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    108\u001b[39m graph_bias = \u001b[38;5;28mself\u001b[39m._create_graph_attention_bias(batch_size)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     tokens = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m tokens = \u001b[38;5;28mself\u001b[39m.norm(tokens)\n\u001b[32m    114\u001b[39m cls_output = tokens[:, \u001b[32m0\u001b[39m, :]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/src/model.py:199\u001b[39m, in \u001b[36mGraphTransformerLayer.forward\u001b[39m\u001b[34m(self, x, attn_bias)\u001b[39m\n\u001b[32m    197\u001b[39m residual = x\n\u001b[32m    198\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m x = residual + \u001b[38;5;28mself\u001b[39m.dropout1(attn_output)\n\u001b[32m    202\u001b[39m residual = x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/projects/graph_transformer_proteomics/CleanedProject/Executable_Project_Code/src/model.py:163\u001b[39m, in \u001b[36mGraphAwareMultiheadAttention.forward\u001b[39m\u001b[34m(self, x, attn_bias)\u001b[39m\n\u001b[32m    160\u001b[39m     scores = scores + attn_bias\n\u001b[32m    162\u001b[39m attn_weights = F.softmax(scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m output = torch.matmul(attn_weights, V)\n\u001b[32m    166\u001b[39m output = output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, L, D)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/envs/AI_ML_DL/lib/python3.11/site-packages/torch/nn/functional.py:1422\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def display_model_architecture():\n",
        "    \"\"\"Display model architecture as formatted table.\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Graph Transformer Classifier Architecture\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n{'Parameter':<30} {'Value':<40}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Embedding Dimension':<30} {config.MODEL['embedding_dim']:<40}\")\n",
        "    print(f\"{'Number of Layers':<30} {config.MODEL['n_layers']:<40}\")\n",
        "    print(f\"{'Attention Heads':<30} {config.MODEL['n_heads']:<40}\")\n",
        "    print(f\"{'Feed-forward Dimension':<30} {config.MODEL['ffn_dim']:<40}\")\n",
        "    print(f\"{'Dropout Rate':<30} {config.MODEL['dropout']:<40}\")\n",
        "    print(f\"{'Graph Bias Scale (learnable)':<30} {config.MODEL['graph_bias_scale']:<40}\")\n",
        "    print(f\"{'Positional Encoding Dim':<30} {config.MODEL['pe_dim']:<40}\")\n",
        "    print(f\"{'Diffusion Kernel Beta':<30} {config.GRAPH_PRIOR['diffusion_beta']:<40}\")\n",
        "    print(\"\\nTraining Parameters:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Learning Rate':<30} {config.TRAINING['learning_rate']:<40}\")\n",
        "    print(f\"{'Weight Decay':<30} {config.TRAINING['weight_decay']:<40}\")\n",
        "    print(f\"{'Batch Size':<30} {config.TRAINING['batch_size']:<40}\")\n",
        "    print(f\"{'Max Epochs':<30} {config.TRAINING['max_epochs']:<40}\")\n",
        "    print(f\"{'Early Stopping Patience':<30} {config.TRAINING['patience']:<40}\")\n",
        "    print(f\"{'Gradient Clipping':<30} {config.TRAINING['grad_clip']:<40}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "if DATA_AVAILABLE:\n",
        "    print(\"Training model from scratch...\")\n",
        "    # Import training functions\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    \n",
        "    # Define training functions inline (or import from training module)\n",
        "    def train_epoch_simple(model, loader, criterion, optimizer, device):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for x, y in tqdm(loader, desc='Training'):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAINING['grad_clip'])\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "        return total_loss / len(loader), accuracy_score(all_labels, all_preds)\n",
        "    \n",
        "    def evaluate_simple(model, loader, criterion, device):\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "                total_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(y.cpu().numpy())\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        return {'loss': total_loss / len(loader), 'accuracy': accuracy}\n",
        "    \n",
        "    # Load data and prior\n",
        "    graph_prior = load_graph_prior(str(PRIOR_PATH))\n",
        "    graph_tensors = get_graph_features_as_tensors(graph_prior, device='cpu')\n",
        "    \n",
        "    data_splits, label_info, scaler = load_and_preprocess_data(str(CSV_PATH), graph_prior['protein_cols'])\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(data_splits)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = GraphTransformerClassifier(\n",
        "        n_proteins=graph_prior['A'].shape[0],\n",
        "        n_classes=label_info['n_classes'],\n",
        "        diffusion_kernel=graph_tensors['K'],\n",
        "        positional_encodings=graph_tensors['PE'],\n",
        "    )\n",
        "    \n",
        "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model parameters: {n_params:,}\")\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.TRAINING['learning_rate'],\n",
        "        weight_decay=config.TRAINING['weight_decay']\n",
        "    )\n",
        "    \n",
        "    # Train for a few epochs (full training would take longer)\n",
        "    print(\"\\nTraining (limited epochs for demo)...\")\n",
        "    model.train()\n",
        "    for epoch in range(min(5, config.TRAINING['max_epochs'])):\n",
        "        train_loss, train_acc = train_epoch_simple(model, train_loader, criterion, optimizer, 'cpu')\n",
        "        val_metrics = evaluate_simple(model, val_loader, criterion, 'cpu')\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Acc={val_metrics['accuracy']:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    checkpoint_dir = PROJECT_ROOT / \"pretrained\"\n",
        "    checkpoint_dir.mkdir(exist_ok=True)\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': {'MODEL': config.MODEL, 'TRAINING': config.TRAINING},\n",
        "        'label_info': label_info,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_dir / \"best_model.pt\")\n",
        "    print(f\"\\nModel saved to {checkpoint_dir / 'best_model.pt'}\")\n",
        "else:\n",
        "    display_model_architecture()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Pretrained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained model (or use newly trained one)\n",
        "try:\n",
        "    model, graph_prior, label_info = load_trained_model(device='cpu')\n",
        "    print(\"✓ Loaded pretrained model\")\n",
        "    print(f\"  Classes: {label_info.get('n_classes', 'unknown')}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"⚠ Pretrained model not found: {e}\")\n",
        "    print(\"  Model will need to be trained first (requires data)\")\n",
        "    model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model is not None and DATA_AVAILABLE:\n",
        "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "    \n",
        "    # Load test data\n",
        "    data_splits, label_info, dataloaders = load_data(return_dataloaders=True, batch_size=32)\n",
        "    test_loader = dataloaders[2]\n",
        "    \n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.numpy())\n",
        "            all_labels.extend(y.numpy())\n",
        "    \n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    \n",
        "    print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"Test F1 (macro): {f1_macro:.4f} ({f1_macro*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"Skipping evaluation - model or data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Interpretability Analysis\n",
        "\n",
        "Run SHAP analysis, attention analysis, PCA baseline, and comparative analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model is not None and DATA_AVAILABLE:\n",
        "    # Set up output directories\n",
        "    plots_dir, results_dir = get_output_dirs(PROJECT_ROOT / \"results\")\n",
        "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(\"Running interpretability analyses...\")\n",
        "    print(\"This may take several minutes...\")\n",
        "    \n",
        "    # Run SHAP analysis\n",
        "    print(\"\\n1. Running SHAP analysis (100 test samples)...\")\n",
        "    try:\n",
        "        from shap_analysis import main as shap_main\n",
        "        shap_main()\n",
        "        print(\"✓ SHAP analysis complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ SHAP analysis failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    # Run attention analysis\n",
        "    print(\"\\n2. Running attention analysis...\")\n",
        "    try:\n",
        "        from attention_analysis import main as attn_main\n",
        "        attn_main()\n",
        "        print(\"✓ Attention analysis complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Attention analysis failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    # Run PCA baseline\n",
        "    print(\"\\n3. Training PCA95 baseline...\")\n",
        "    try:\n",
        "        from pca_baseline import main as pca_main\n",
        "        pca_main()\n",
        "        print(\"✓ PCA baseline complete\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ PCA baseline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print(\"\\n✓ All interpretability analyses complete\")\n",
        "    print(f\"Results saved to: {results_dir}\")\n",
        "else:\n",
        "    print(\"Skipping interpretability - model or data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Result Figures\n",
        "\n",
        "Generate all figures matching the results section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model is not None and DATA_AVAILABLE:\n",
        "    # Load pre-computed results\n",
        "    plots_dir, _ = get_output_dirs(PROJECT_ROOT / \"results\")\n",
        "    \n",
        "    # Check if results exist\n",
        "    shap_results = plots_dir / \"SHAP_Plots\" / \"top_proteins.json\"\n",
        "    pca_results = plots_dir / \"PCA_Cox_Plots\" / \"top_proteins.json\"\n",
        "    attn_results = plots_dir / \"Attention_Plots\" / \"attention_stats.txt\"\n",
        "    \n",
        "    if shap_results.exists() and pca_results.exists():\n",
        "        print(\"Loading analysis results...\")\n",
        "        \n",
        "        # Load SHAP results\n",
        "        with open(shap_results, 'r') as f:\n",
        "            shap_data = json.load(f)\n",
        "        shap_proteins = [p['protein'] for p in shap_data]\n",
        "        shap_importance = np.array([p['importance'] for p in shap_data])\n",
        "        \n",
        "        # Load PCA results\n",
        "        with open(pca_results, 'r') as f:\n",
        "            pca_data = json.load(f)\n",
        "        pca_proteins = [p['protein'] for p in pca_data]\n",
        "        pca_importance = np.array([p['importance'] for p in pca_data])\n",
        "        \n",
        "        # Load PPI network\n",
        "        prior_data = np.load(DEFAULT_PATHS['prior'], allow_pickle=True)\n",
        "        A = prior_data['A']\n",
        "        all_proteins = prior_data['protein_cols'].tolist()\n",
        "        \n",
        "        print(f\"Loaded {len(shap_proteins)} SHAP proteins\")\n",
        "        print(f\"Loaded {len(pca_proteins)} PCA proteins\")\n",
        "        print(f\"PPI network: {A.shape[0]} proteins\")\n",
        "        \n",
        "        # Generate comparison plots\n",
        "        comparison_dir = plots_dir / \"Model_Comparison_Plots\"\n",
        "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Top 20 overlap\n",
        "        shap_top20 = set(shap_proteins[:20])\n",
        "        pca_top20 = set(pca_proteins[:20])\n",
        "        overlap = shap_top20 & pca_top20\n",
        "        \n",
        "        print(f\"\\nTop 20 Overlap: {len(overlap)}/{20} ({len(overlap)/20*100:.0f}%)\")\n",
        "        print(f\"Overlap proteins: {sorted(overlap)}\")\n",
        "        \n",
        "        # SHAP vs Attention correlation (if attention results exist)\n",
        "        if attn_results.exists():\n",
        "            print(\"\\n✓ All analyses complete - results available in plots/\")\n",
        "        else:\n",
        "            print(\"\\n⚠ Some analyses may still be running\")\n",
        "    else:\n",
        "        print(\"Analysis results not yet available - run interpretability cells first\")\n",
        "else:\n",
        "    print(\"Skipping result generation - model or data not available\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI_ML_DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
